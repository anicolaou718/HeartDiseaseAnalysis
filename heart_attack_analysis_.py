# -*- coding: utf-8 -*-
"""Heart Attack Analysis .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FKORZzZXA88agrehDsXhas-6JIgz8YHQ
"""

import numpy as np 
import pandas as pd 
import seaborn as sns 
import matplotlib.pyplot as plt 

from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import RobustScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_curve, auc, roc_auc_score, accuracy_score


from google.colab import drive

#drive.mount('/content/gdrive');

df = pd.read_csv('framingham.csv') 
#df = pd.read_csv('/content/gdrive/MyDrive/data_mining/framingham.csv')
df.head(20)

"""Pre-Processing Data"""

#Checks for duplicates
duplicate_df = df[df.duplicated()]
duplicate_df

df.isna().sum()
null = df[df.isna().any(axis=1)]
null

pd.options.display.max_columns = None
pd.options.display.max_rows = None

"""Handling Null Values and Ennumerating Strings/Objects by converting to Binary"""

#Missing values in Education
eduMiss=df[df['education'].isnull()].index
eduMiss

#Drop All null values in Education.
df=df.drop(eduMiss)

df.info()

#Turning Yes/No values to Binary
df = df.replace(['Yes'],1)
df = df.replace(['No'],0)

#Turning Male and Female into Binary
df['Sex'] = (df['Sex'] == 'male').astype(int)

df.info()

# Null values in Cigs per day are non smokers.
cigIndex = df[df['cigsPerDay'].isnull()].index
cigIndex

smoke_status = []
for i in cigIndex:
   smoke_status.append(df['currentSmoker'][i])

smoke_status

smokers = df[df['currentSmoker'] == 1].index
smokers

numOfcigarettes = []
for i in smokers:
     if df['cigsPerDay'][i] != 'nan':
        numOfcigarettes.append(df['cigsPerDay'][i])

len(numOfcigarettes)

import statistics
medianSmoker = statistics.median(numOfcigarettes)
medianSmoker

df['cigsPerDay'] = df['cigsPerDay'].fillna(medianSmoker)

df.isnull().sum()

BPmissing= df[df['BPMeds'].isnull()].index
BPmissing

for i in BPmissing:
    if ( df['sysBP'][i] > 140 or df['diaBP'][i] > 90 ):
        df.loc[i,'BPMeds'] = 1.0  
    else:
        df.loc[i,'BPMeds'] = 0.0

df['totChol'] = df['totChol'].fillna(round(df['totChol'].mean()))

df['BMI'] = df['BMI'].fillna(df['BMI'].mean())

df['glucose'] = df['glucose'].fillna(round(df['glucose'].mean()))

HRmissing = df[df['heartRate'].isnull()].index
HRmissing

df['heartRate'] = df['heartRate'].fillna(round(df['heartRate'].mean()))

df.isnull().sum()

"""EDA : Exploratory Data Analysis Begins"""

#Education Doesn't Have Any Significance. So, it will be dropped
df.drop('education', axis=1, inplace=True)

#Prints 2nd Row
print( 'Shape of Heart Prediction: {}'.format(df.shape))
print (df.loc[1])

df.describe()

df.tail()

#Histograms for every column
fig = plt.figure(figsize = (15,20))
ax = fig.gca()
df.hist(ax = ax)

#IDENTIFY AND REMOVE OUTLIERS
from matplotlib.cbook import boxplot_stats
from scipy import stats

#register max number of outliers
totalFTWithOutliers = []

print("Data's initial shape", '(4135,15)')

# helpers
def isInsideList(list, value):
  for i in list:
    return True

  return False

def removeOutliers(valuesToDelete, outliers):
  # valuesToDelete = values @ df[column]
  # outliers list of outliers for ^^ 
  for index, value in valuesToDelete.items():
    if value in outliers:
      # print('Attempting to delete this value =>', value, 'at this index' , index)
      df.drop([index],axis=0,inplace=True)


# graph outliers for all columns in differt graphs for better visualization
for i in df.columns: 
  outliers = [y for stat in boxplot_stats(df[i]) for y in stat['fliers']]
  if len(outliers) > 1:
    totalFTWithOutliers.append(len(outliers))
    plt.rcParams["figure.figsize"] = [4.50, 2.50]
    plt.rcParams["figure.autolayout"] = True  
    plt.figure() # make sure we print a box per variable
    sns.boxplot(x=df[i], width=0.7)
    print("Total Outliers for " + i + '=' , len(outliers))
    print("Outliers for " + i + '=' , np.unique(outliers), "\n")
    
# remove outliers where the count isn't greater than 10% (41.35) of the total population of the dataframe
# the rest was left as is as we believe these represent natural variations in our dataset
    # if (len(outliers) < 41.35) & (isInsideList(outliers, df[i])):
      # removeOutliers(df[i], np.unique(outliers))


print("Data's final shape without outliers", df.shape)
print("Total outliers removed= ", 4135 - df.shape[0])

#Histogram of Target Variable
sns.countplot(x='TenYearCHD',data=df)

sns.pairplot(data=df)

pd.crosstab(df.age,df.TenYearCHD).plot(kind="bar",figsize=(20,6))
plt.title('Heart Disease Frequency for Ages')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(10,10))
sns.heatmap(df.corr(),annot=True,fmt='.1f')
plt.show() # Orange Boxes show Higher Correlations Between the Variables

plt.figure(figsize=(10,5))
sns.pointplot(x=df.age,y=df.prevalentStroke,color='red',alpha=0.8)
plt.xlabel('Age',fontsize = 15,color='blue')
plt.xticks(rotation=45)
plt.ylabel('Stroke Prevalence',fontsize = 15,color='blue')
plt.title('The Prevalence of Stroke based on Age',fontsize = 15,color='blue')
plt.grid()
plt.show()

plt.figure(figsize=(10,5))
sns.pointplot(x=df.totChol,y=df.prevalentStroke,color='red',alpha=0.8)
plt.xlabel('Age',fontsize = 15,color='blue')
plt.xticks(rotation=45)
plt.ylabel('Stroke Prevalence',fontsize = 15,color='blue')
plt.title('The Prevalence of Stroke based on Age',fontsize = 15,color='blue')
plt.grid()
plt.show()

plt.figure(figsize=(10,5))
sns.pointplot(x=df.cigsPerDay,y=df.prevalentStroke,color='red',alpha=0.8)
plt.xlabel('Cigs per Day',fontsize = 15,color='blue')
plt.xticks(rotation=45)
plt.ylabel('Stroke Prevalence',fontsize = 15,color='blue')
plt.title('The Frequency of Stroke Prevalence based on the Amount of Cigs per Day',fontsize = 15,color='blue')
plt.grid()
plt.show()

samp=df.sample(frac=0.5, replace=True, random_state=1) # Took 50% sample of original dataset
#Constructing Scatter plot: Original thought prior to FHS was that BMI was affected by age
y= samp['BMI']
x= samp['age']
plt.scatter(x, y)
plt.rcParams.update({'figure.figsize':(10,8), 'figure.dpi':100})
plt.title('The impact of BMI based on Age')
plt.xlabel('BMI')
plt.ylabel('Age')
plt.show()

#Constructing Scatter plot: Original thought prior to FHS was that Hypertension was affected by age
plt.figure(figsize=(10,5))
sns.pointplot(x=df.age,y=df.prevalentHyp,color='red',alpha=0.8)
plt.xlabel('Age',fontsize = 15,color='blue')
plt.xticks(rotation=45)
plt.ylabel('HyperTension Prevalence',fontsize = 15,color='blue')
plt.title('Age vs HyperTension Prevalence',fontsize = 15,color='blue')
plt.grid()
plt.show()

plt.scatter(x=df.age[df.TenYearCHD==1], y=df.heartRate[(df.TenYearCHD==1)], c="red")
plt.scatter(x=df.age[df.TenYearCHD==0], y=df.heartRate[(df.TenYearCHD==0)])
plt.legend(["CHD", "No CHD"])
plt.xlabel("Age")
plt.title('The Prevalence of Heart Disease based on Age and Heart Rate')
plt.ylabel("Max Heart Rate")
plt.show()

pd.crosstab(df.prevalentHyp,df.TenYearCHD).plot(kind="bar",figsize=(15,6),color=['#1CA53B','#AA1111' ])
plt.title('Frequency of Heart Disease based on Hypertension')
plt.xlabel('Hypertension')
plt.xticks(rotation=0)
plt.legend(["No Disease", "Have Disease"])
plt.ylabel('Frequency of Heart Disease Diagnosis')
plt.show()

pd.crosstab(df.Sex,df.TenYearCHD).plot(kind="bar",figsize=(15,6),color=['#1CA53B','#AA1111' ])
plt.title('Frequency of Heart Disease based on Sex')
plt.xlabel('Sex (F=0,M=1)')
plt.xticks(rotation=0)
plt.legend(["No Disease", "Have Disease"])
plt.ylabel('Frequency of Heart Disease Diagnosis')
plt.show()

df.dtypes

pd.crosstab(df.currentSmoker,df.TenYearCHD).plot(kind="bar",figsize=(15,6),color=['#1CA53B','#AA1111' ])
plt.title('Frequency of Heart Disease based on Smoking')
plt.xlabel('Smoking Status')
plt.xticks(rotation=0)
plt.legend(["No Disease", "Have Disease"])
plt.ylabel('Frequency of Heart Disease Diagnosis')
plt.show()

pd.crosstab(df.prevalentStroke,df.TenYearCHD).plot(kind="bar",figsize=(15,6),color=['#1CA53B','#AA1111' ])
plt.title('Frequency of Heart Disease based on Stroke Status')
plt.xlabel('Stroke Status')
plt.xticks(rotation=0)
plt.legend(["No Disease", "Have Disease"])
plt.ylabel('Frequency of Heart Disease Diagnosis')
plt.show()

pd.crosstab(df.cigsPerDay,df.TenYearCHD).plot(kind="bar",figsize=(15,6),color=['#1CA53B','#AA1111' ])
plt.title('Frequency of Heart Disease based on Smoking Habits')
plt.xlabel('Number of Cigarettes')
plt.xticks(rotation=0)
plt.legend(["No Disease", "Have Disease"])
plt.ylabel('Frequency of Heart Disease Diagnosis')
plt.show()

pd.crosstab(df.BPMeds,df.TenYearCHD).plot(kind="bar",figsize=(15,6),color=['cyan','#BD2281' ])
plt.title('Heart Disease Frequency based on usage of BP Treatment')
plt.xlabel('Blood Pressure Medication (0 = Female, 1 = Male)')
plt.xticks(rotation=0)
plt.legend(["Haven't Disease", "Have Disease"])
plt.ylabel('Frequency')
plt.show()

"""Creating Logistic Regression Model: 
Part 1 - Splitting Data
"""

#Creating Dependent and Independent Variables : TenYrCHD is the target value
X = df.drop(['TenYearCHD'], axis = 1)
Y = df.TenYearCHD.values

x = (X - np.min(X)) / (np.max(X) - np.min(X)).values

# Data Split
X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size = .2, random_state=10) #split the data 80-20

# Fitting Logistic Model
model = LogisticRegression(solver='liblinear', C=10.0,random_state=1)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print('Logistic regression Model Accuracy Rate: {:.2f}'.format(model.score(X_test, y_test)))

from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(y_test, y_pred)) # True negatives in the upper-left position, False negatives in the lower-left positionm, False positives in the upper-right position, True positives in the lower-right position
print(classification_report(y_test, y_pred))

model.intercept_ # y intercept

model.coef_ # slope

model.predict_proba(X) # Predicts probability that the prediction is either 0 or 1

model.predict_proba(X).size

y_pred # predictions

model.score(X, Y)

cm = confusion_matrix(y_test, y_pred)

fig, ax = plt.subplots(figsize=(8, 8))
ax.imshow(cm)
ax.grid(False)
ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))
ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))
ax.set_ylim(1.5, -0.5)
for i in range(2):
    for j in range(2):
        ax.text(j, i, cm[i, j], ha='center', va='center', color='red')
plt.show()

from sklearn.metrics import roc_curve, auc #for model evaluation

fpr, tpr, thresholds = roc_curve(y_test, y_pred)

fig, ax = plt.subplots()
ax.plot(fpr, tpr)
ax.plot([0, 1], [0, 1], transform=ax.transAxes, ls="--", c=".3")
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.rcParams['font.size'] = 12
plt.title('ROC curve for diabetes classifier')
plt.xlabel('False Positive Rate (1 - Specificity)')
plt.ylabel('True Positive Rate (Sensitivity)')
plt.grid(True)

auc(fpr, tpr)

# Check for imbalance bc of low auc score and low rate of true positive ones
df.TenYearCHD.value_counts().plot.pie(autopct='%.2f')

pip install imbalanced-learn

count_class_0, count_class_1 = df.TenYearCHD.value_counts()

# Divide by class
df_class_0 = df[df['TenYearCHD'] == 0]
df_class_1 = df[df['TenYearCHD'] == 1]

df_class_0_under = df_class_0.sample(count_class_1)
df_test_under = pd.concat([df_class_0_under, df_class_1], axis=0)

print('Random under-sampling:')
print(df_test_under.TenYearCHD.value_counts())

df_test_under.TenYearCHD.value_counts().plot(kind='bar', title='Count (target)');

# Try to undersample Data to fix imbalance and retry predictive models
from imblearn.under_sampling import RandomUnderSampler
ru= RandomUnderSampler(random_state=42, replacement=True)
x_ru,y_ru= ru.fit_resample(X,Y)
print("Original Df shape : ", y.shape)
print('Current Df shape: ', y_ru.shape)

X_train, X_test, y_train, y_test = train_test_split(x_ru,y_ru, test_size = .2, random_state=10) #split the data 80-20

model = LogisticRegression(solver='liblinear', C=10.0,random_state=1)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print('Logistic regression Model Accuracy Rate: {:.2f}'.format(model.score(X_test, y_test)))

fpr, tpr, thresholds = roc_curve(y_test, y_pred)

fig, ax = plt.subplots()
ax.plot(fpr, tpr)
ax.plot([0, 1], [0, 1], transform=ax.transAxes, ls="--", c=".3")
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.rcParams['font.size'] = 12
plt.title('ROC curve for Heart Disease classifier')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.grid(True)

auc(fpr, tpr) # Accuracy was lowered but it was more accurate

cm = confusion_matrix(y_test, y_pred)

fig, ax = plt.subplots(figsize=(8, 8))
ax.imshow(cm)
ax.grid(False)
ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))
ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))
ax.set_ylim(1.5, -0.5)
for i in range(2):
    for j in range(2):
        ax.text(j, i, cm[i, j], ha='center', va='center', color='red')
plt.show()

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier()
rf.fit(X_train,y_train)

rf_pred = rf.predict(X_test)
print('AUC score is: ', roc_auc_score(y_test,rf_pred))
print('The Accuracy score is: ', accuracy_score(y_test, rf_pred))

"""Target Variable Data is Unbalanced : Undersampling vs Oversampling"""

#UnderSampling
from imblearn.under_sampling import NearMiss
nm= NearMiss()
x_nm,y_nm=nm.fit_resample(X,Y)
print("Original Df shape: ", Y.shape)
print('Current Df shape: ', y_nm.shape)

#OVersampling
from imblearn.over_sampling import SMOTE
smote=SMOTE()
x_smote,y_smote= smote.fit_resample(X,Y)
print("Original Df shape : ", Y.shape)
print('Current Df shape: ', y_smote.shape)

"""Testing Algorithms(LogReg, SVC, Forest) and Evaluations After SMOTE

Data Splt : 80%/20%
"""

X_train, X_test, y_train, y_test = train_test_split(x_smote,y_smote, test_size = .2, random_state=10) #split the data 80-20

"""Logistic Regressions"""

model = LogisticRegression(solver='liblinear', C=10.0,random_state=1)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print('Logistic regression Model Accuracy Rate: {:.2f}'.format(model.score(X_test, y_test)))

from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(y_test, y_pred)) # True negatives in the upper-left position, False negatives in the lower-left positionm, False positives in the upper-right position, True positives in the lower-right position
print(classification_report(y_test, y_pred))

"""Poly SVC with C=100"""

from sklearn.svm import SVC
poly_svc=SVC(kernel='poly', C=100.0)  #C parameter: the degree of correct classification that the algorithm has to meet.
poly_svc.fit(X_train,y_train)
y_pred=poly_svc.predict(X_test)
print('Accuracy Score with poly kernel and C=100.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))

from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(y_test, y_pred)) # True negatives in the upper-left position, False negatives in the lower-left positionm, False positives in the upper-right position, True positives in the lower-right position
print(classification_report(y_test, y_pred))

"""Random Forest """

rf = RandomForestClassifier()
rf.fit(X_train,y_train)

rf_pred = rf.predict(X_test)
print('AUC score is: ', roc_auc_score(y_test,rf_pred))
print('The Accuracy score is: ', accuracy_score(y_test, rf_pred))

from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(y_test, rf_pred)) # True negatives in the upper-left position, False negatives in the lower-left positionm, False positives in the upper-right position, True positives in the lower-right position
print(classification_report(y_test, rf_pred))

fpr, tpr, thresholds = roc_curve(y_test, rf_pred)

fig, ax = plt.subplots()
ax.plot(fpr, tpr)
ax.plot([0, 1], [0, 1], transform=ax.transAxes, ls="--", c=".3")
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.rcParams['font.size'] = 12
plt.title('ROC curve for Heart Disease classifier')
plt.xlabel('False Positive Rate (1 - Specificity)')
plt.ylabel('True Positive Rate (Sensitivity)')
plt.grid(True)

cm = confusion_matrix(y_test, rf_pred)

fig, ax = plt.subplots(figsize=(8, 8))
ax.imshow(cm)
ax.grid(False)
ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))
ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))
ax.set_ylim(1.5, -0.5)
for i in range(2):
    for j in range(2):
        ax.text(j, i, cm[i, j], ha='center', va='center', color='red')
plt.show()